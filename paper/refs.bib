@inproceedings{morris2020textattack,
  title={TextAttack: A Framework for Adversarial Attacks, Data Augmentation, and Adversarial Training in NLP},
  author={Morris, John X. and et al.},
  booktitle={EMNLP},
  year={2020}
}

@inproceedings{gao2018deepwordbug,
  title={Black-box Generation of Adversarial Text Sequences to Evade Deep Learning Classifiers},
  author={Gao, Ji and et al.},
  booktitle={IEEE Security and Privacy Workshops},
  year={2018}
}

@inproceedings{sanh2019distilbert,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and et al.},
  booktitle={NeurIPS Workshop},
  year={2019}
}

@article{almeida2011sms,
  title={Contributions to the Study of SMS Spam Filtering: New Collection and Results},
  author={Almeida, Tiago and Hidalgo, Jos{\'e} Mar{\'i}a and Yamakami, Akebo},
  journal={Proceedings of ACM Symposium on Document Engineering},
  year={2011}
}

@misc{spamassassin,
  title={SpamAssassin Public Corpus},
  author={The Apache Software Foundation},
  year={2006},
  howpublished={https://spamassassin.apache.org/publiccorpus/}
}

@article{gururangan2020don,
  title={Don't Stop Pretraining: Adapt Language Models to Domains and Tasks},
  author={Gururangan, Suchin and et al.},
  journal={ACL},
  year={2020}
}

@article{guzella2009review,
  title={A review of machine learning approaches to spam filtering},
  author={Guzella, Thiago S. and Caminhas, Walmir M.},
  journal={Expert Systems with Applications},
  year={2009}
}

@inproceedings{blitzer2007biographies,
  title={Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification},
  author={Blitzer, John and Dredze, Mark and Pereira, Fernando},
  booktitle={ACL},
  year={2007}
}

@inproceedings{jin2020textfooler,
  title={Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment},
  author={Jin, Di and Jin, Zhijing and Zhou, Joey Tianyi and Szolovits, Peter},
  booktitle={AAAI},
  year={2020}
}

@inproceedings{lee2022dedup,
  title={Deduplicating Training Data Makes Language Models Better},
  author={Lee, Katherine and et al.},
  booktitle={ACL},
  year={2022}
}
