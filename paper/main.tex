\documentclass[11pt]{article}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{geometry}
\geometry{margin=1in}
\graphicspath{{figs/}}

\title{DedupShift: Credible Cross-Domain Benchmarking for Risky Text Detection}
\author{}
\date{}

\begin{document}
\maketitle

\begin{abstract}
We present a reproducible benchmark for risky text detection (spam/phish/fraud-like) across SMS and email domains. We report strong in-domain baselines (F1 up to 0.99) but substantial cross-domain degradation, and show that even a neural anchor baseline (DistilBERT fine-tuning) does not eliminate domain shift. We introduce DedupShift, a deduplicated split protocol that reduces leakage from template-heavy corpora and changes robustness estimates. Our suite includes perturbation-based robustness, normalization defense ablation, TextAttack sanity checks, and domain-shift diagnostics.
\end{abstract}

\section{Introduction}
We study risky text detection under domain shift and adversarial perturbations. Our contributions are: (1) a two-domain benchmark with unified schema and reproducible pipeline; (2) DedupShift, a deduplicated split protocol to control leakage; (3) a robustness suite with defense ablations and a neural anchor baseline (DistilBERT-FT) for modern reference.

\section{Benchmark Setup}
We use SMS (UCI SMSSpamCollection) and SpamAssassin corpora with 80/10/10 stratified splits. DedupShift removes exact and near-duplicates (SimHash) before re-splitting to reduce leakage and template overlap.

\section{Baselines}
We report TF-IDF word LR, TF-IDF character SVM, MiniLM+LR, and DistilBERT-FT as a neural anchor baseline. We add two lightweight improvement baselines: (i) \textbf{AugTrain}, which applies training-time augmentation (obfuscation, paraphrase-like substitutions, and normalization) to spam samples; and (ii) \textbf{CORAL} domain alignment on MiniLM embeddings to reduce covariate shift. DistilBERT uses max length 128, batch 8, 2 epochs, seed 0.

\section{Results}
\input{tables/cross_domain_table_dedup.tex}
\input{tables/dedup_effect.tex}
\input{tables/domain_shift_stats.tex}
\input{tables/textattack_table.tex}

\begin{figure}[t]
\centering
\includegraphics[width=0.95\linewidth]{Pipeline.png}
\caption{Benchmark pipeline overview.}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.95\linewidth]{fig_robustness_delta_dedup.png}
\caption{Robustness deltas on deduplicated splits.}
\end{figure}

\section{Hyperparameter Sensitivity and Multi-Seed Robustness}
\textbf{DedupShift threshold analysis.} We analyze the sensitivity of DedupShift to SimHash Hamming thresholds (2, 3, 4). Lower thresholds remove more near-duplicates but may over-deduplicate; higher thresholds preserve more data but allow more leakage. We report deduplication rates and model F1 scores across thresholds to quantify this trade-off.

\input{tables/sensitivity_dedup_threshold.tex}

\begin{figure}[t]
\centering
\includegraphics[width=0.95\linewidth]{fig_sensitivity_dedup_threshold.png}
\caption{DedupShift sensitivity: F1 and dedup rate vs. Hamming threshold.}
\end{figure}

\textbf{DistilBERT multi-seed training.} To ensure training stability and reduce seed-dependent variability, we train DistilBERT with seeds 0, 1, 2 and report mean $\pm$ std F1 scores. This demonstrates reproducibility and robustness of the neural baseline.

\input{tables/distilbert_multiseed.tex}

\section{Related Work}
\textbf{Spam/phish detection baselines.} Classic statistical filters and linear models with TF-IDF features remain strong baselines for spam detection \cite{guzella2009review,almeida2011sms}. Recent work adopts BERT-family models for text classification \cite{sanh2019distilbert}.

\textbf{Domain shift in NLP.} Cross-domain degradation is well-documented in sentiment and text classification; adaptation and continued pretraining are common remedies \cite{blitzer2007biographies,gururangan2020don}.

\textbf{Dataset leakage and near-duplicates.} Near-duplicate contamination can inflate evaluation scores; deduplication has been shown to improve validity in NLP benchmarks \cite{lee2022dedup}.

\textbf{Adversarial text and robustness.} Character-level and synonym-based attacks reveal brittleness of text classifiers; TextAttack provides standardized evaluation recipes \cite{gao2018deepwordbug,morris2020textattack,jin2020textfooler}.

\section{Threats to Validity and Limitations}
\textbf{Domain coverage.} We evaluate only SMS and email; broader domains and languages may exhibit different shift patterns.

\textbf{DedupShift approximation.} DedupShift removes exact and SimHash near-duplicates but may miss semantic paraphrases; our sensitivity analysis (Section~\ref{sec:sensitivity}) explores threshold effects. Lower thresholds increase deduplication but may harm model performance; higher thresholds preserve more data but allow residual leakage.

\textbf{Seed variance.} While we report mean $\pm$ std for DistilBERT multi-seed training, classical baselines (TF-IDF, MiniLM) are deterministic or have minimal seed variance and are reported at seed 0.

\textbf{Defense scope.} Normalization and AugTrain are heuristic and may behave differently across formats or multilingual text.

\textbf{Adversarial evaluation.} TextAttack is run on subsets and is a diagnostic, not an exhaustive adversarial guarantee.

\section{Reproducibility and Artifacts}
We release processing scripts, fixed seeds, and Makefile targets for one-command reproduction (\texttt{make paper\_repro}). This target sequences all data preprocessing, deduplication (with default $h_{\text{thresh}}=3$), model training, sensitivity analyses, robustness evaluation, and LaTeX table generation. Key artifacts include cross-domain tables, robustness matrices, DedupShift reports, TextAttack summaries, domain-shift diagnostics, hyperparameter sensitivity curves, and multi-seed neural baseline results.

\bibliographystyle{plain}
\bibliography{refs}
\end{document}
