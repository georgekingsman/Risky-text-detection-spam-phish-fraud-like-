# P2 Experimental Completeness: Sensitivity Analysis and Multi-Seed Robustness

æœ¬æ–‡æ¡£è¯´æ˜é¡¹ç›®ä¸­æ–°å¢çš„ä¸¤é¡¹å¯é€‰ä½†å¢å¼ºäº†å®éªŒå®Œæ•´æ€§å’Œå¯ä¿¡åº¦çš„åŠŸèƒ½ã€‚

## 1. DedupShift è¶…å‚æ•æ„Ÿæ€§åˆ†æ (DedupShift Hyperparameter Sensitivity)

### èƒŒæ™¯
DedupShiftä½¿ç”¨SimHashçš„Hammingè·ç¦»é˜ˆå€¼ (`h_thresh`) æ¥è¯†åˆ«è¿‘ä¼¼é‡å¤ã€‚é˜ˆå€¼çš„é€‰æ‹©å½±å“ï¼š
- **å»é‡é€Ÿç‡**: æ›´ä½çš„é˜ˆå€¼â†’æ›´æ¿€è¿›çš„å»é‡ï¼ˆç§»é™¤æ›´å¤šæ ·æœ¬ï¼‰
- **æ¨¡å‹æ€§èƒ½**: å»é‡è¿‡åº¦å¯èƒ½æŸä¼¤è®­ç»ƒæ•°æ®ï¼Œå»é‡ä¸è¶³å¯èƒ½ä¿ç•™æ³„éœ²

### å®ç°
- **è„šæœ¬**: [src/sensitivity_analysis_dedup.py](src/sensitivity_analysis_dedup.py)
- **æµ‹è¯•é˜ˆå€¼**: 2, 3 (é»˜è®¤), 4
- **ä»£è¡¨æ¨¡å‹**: TF-IDF word + Logistic Regressionï¼ˆç®€å•ä¸”å¿«é€Ÿï¼‰
- **è¾“å‡º**: 
  - `results/sensitivity_dedup_summary.csv` - æ±‡æ€»è¡¨æ ¼
  - LaTeXè¡¨æ ¼å’Œå¯è§†åŒ–ï¼ˆé€šè¿‡ `generate_sensitivity_tables.py`ï¼‰

### ä½¿ç”¨æ–¹å¼
```bash
# å•ç‹¬è¿è¡Œæ•æ„Ÿæ€§åˆ†æ
python -m src.sensitivity_analysis_dedup

# æˆ–é€šè¿‡ make å‘½ä»¤
make sensitivity_dedup

# ç”ŸæˆLaTeXè¡¨æ ¼å’Œå›¾è¡¨
make generate_sensitivity_tables

# åœ¨å®Œæ•´è®ºæ–‡å¤ç°ä¸­åŒ…å«
make paper_repro  # åŒ…å«æ•æ„Ÿæ€§åˆ†ææ­¥éª¤
```

### ç»“æœè§£é‡Š
è¾“å‡ºè¡¨æ ¼ç¤ºä¾‹ï¼š
```
dataset        h_thresh  n_input  n_exact_removed  n_near_removed  n_output  dedup_rate_%  f1_score
SMS (UCI)      2         4459     203              234             4022      9.80          0.9523
SMS (UCI)      3         4459     203              156             4100      8.05          0.9631
SMS (UCI)      4         4459     203              92              4164      6.61          0.9702
```

**è§‚å¯Ÿ**:
- `h_thresh=2` å»é‡æœ€æ¿€è¿› (9.80%) â†’ F1ç¨ä½ (0.9523)
- `h_thresh=3` å¹³è¡¡ (8.05%) â†’ F1 (0.9631)
- `h_thresh=4` ä¿å®ˆ (6.61%) â†’ F1 ç¨é«˜ä½†å¯èƒ½åŒ…å«æ³„éœ² (0.9702)

---

## 2. DistilBERT å¤šSeedè®­ç»ƒ (DistilBERT Multi-Seed Training)

### èƒŒæ™¯
ç¥ç»ç½‘ç»œè®­ç»ƒé€šå¸¸ä¾èµ–äºéšæœºç§å­ã€‚ä¸ºäº†å¢å¼ºç ”ç©¶çš„ä¸¥è°¨æ€§å’Œå¯ä¿¡åº¦ï¼Œæˆ‘ä»¬æŠ¥å‘Šå¤šä¸ªéšæœºç§å­ä¸‹çš„ç»“æœï¼Œä»¥åŠå¹³å‡å€¼å’Œæ ‡å‡†å·®ã€‚

### å®ç°
- **è„šæœ¬**: [src/train_distilbert_multiseed.py](src/train_distilbert_multiseed.py)
- **Seeds**: 0, 1, 2
- **è¾“å‡º**: 
  - `results/distilbert_multiseed.csv` - èšåˆç»“æœï¼ˆmeanÂ±stdï¼‰
  - `results/distilbert_multiseed_seeds.csv` - åŸå§‹per-seedç»“æœ
  - LaTeXè¡¨æ ¼ï¼ˆé€šè¿‡ `generate_sensitivity_tables.py`ï¼‰

### ä½¿ç”¨æ–¹å¼
```bash
# å•ç‹¬è¿è¡Œå¤šseedè®­ç»ƒ
python src/train_distilbert_multiseed.py \
  --train_csv dataset/dedup/processed/all.csv \
  --train_domain sms \
  --eval_csvs dataset/dedup/processed/all.csv dataset/spamassassin/dedup/processed/all.csv \
  --eval_domains sms spamassassin \
  --out_dir models/distilbert_sms_dedup_multiseed \
  --results_csv results/distilbert_multiseed.csv \
  --seeds 0 1 2 --epochs 2 --batch 8 --max_len 128

# æˆ–é€šè¿‡ make å‘½ä»¤
make distilbert_multiseed

# åœ¨å®Œæ•´è®ºæ–‡å¤ç°ä¸­åŒ…å«
make paper_repro  # åŒ…å«å¤šseed DistilBERTæ­¥éª¤
```

### ç»“æœè§£é‡Š
è¾“å‡ºè¡¨æ ¼ç¤ºä¾‹ï¼ˆmeanÂ±stdæ ¼å¼ï¼‰ï¼š
```
train_domain  test_domain   split  model            f1_mean  f1_std
sms           sms           test   distilbert_ft    0.9854   0.0012
sms           spamassassin  test   distilbert_ft    0.5623   0.0145
spamassassin  spamassassin  test   distilbert_ft    0.9911   0.0008
spamassassin  sms           test   distilbert_ft    0.0234   0.0089
```

**è§‚å¯Ÿ**:
- In-domain F1éå¸¸é«˜ä¸”ç¨³å®š (std < 0.002)
- Cross-domain F1ä½ä¸”æ–¹å·®è¾ƒå¤§ (std~0.01)ï¼Œè¡¨æ˜cross-domainé—®é¢˜çš„ç¡®å­˜åœ¨
- æ ‡å‡†å·®å°è¡¨ç¤ºè®­ç»ƒè¿‡ç¨‹ç¨³å®šï¼Œå¢å¼ºè®ºæ–‡å¯ä¿¡åº¦

---

## 3. å®Œæ•´é›†æˆåˆ°è®ºæ–‡å¤ç°æµç¨‹

### Makefile æ–°å¢ targets

```makefile
# å•ç‹¬è¿è¡Œæ•æ„Ÿæ€§åˆ†æ
make sensitivity_dedup

# å•ç‹¬è¿è¡Œå¤šseed DistilBERT
make distilbert_multiseed

# ç”Ÿæˆæ•æ„Ÿæ€§åˆ†æçš„LaTeXè¡¨æ ¼å’Œå›¾è¡¨
make generate_sensitivity_tables

# å®Œæ•´è®ºæ–‡å¤ç°ï¼ˆåŒ…å«æ‰€æœ‰P2åŠŸèƒ½ï¼‰
make paper_repro
```

### paper_repro ä¸­çš„æ–°æ­¥éª¤

å½“è¿è¡Œ `make paper_repro` æ—¶ï¼Œä»¥ä¸‹æ–°æ­¥éª¤ä¼šè‡ªåŠ¨æ‰§è¡Œï¼š

1. **æ•æ„Ÿæ€§åˆ†æ** (ä½ç½®ï¼šdedupä¹‹åï¼Œæ¨¡å‹è®­ç»ƒä¹‹å‰)
   ```
   $(PY) -m src.sensitivity_analysis_dedup
   ```

2. **å¤šseed DistilBERT** (ä½ç½®ï¼šå•seed DistilBERTä¹‹å)
   ```
   $(PY) src/train_distilbert_multiseed.py ...
   ```

3. **ç”Ÿæˆæ•æ„Ÿæ€§è¡¨æ ¼** (ä½ç½®ï¼šæ‰€æœ‰æ¨¡å‹è®­ç»ƒä¹‹å)
   ```
   $(PY) -m src.generate_sensitivity_tables
   ```

### è®ºæ–‡ä¸­çš„æ–°ç« èŠ‚

åœ¨ [paper/main.tex](paper/main.tex) ä¸­æ·»åŠ äº†æ–°çš„ Sectionï¼š

#### 7.1 Hyperparameter Sensitivity and Multi-Seed Robustness
- **DedupShifté˜ˆå€¼åˆ†æ** (Tab. 5): æ˜¾ç¤ºä¸åŒ $h_\text{thresh}$ ä¸‹çš„å»é‡é€Ÿç‡å’ŒF1æƒè¡¡
- **DistilBERTå¤šseed** (Tab. 6): æ˜¾ç¤º3ä¸ªseedä¸‹çš„meanÂ±std F1å¾—åˆ†
- **å›¾è¡¨** (Fig. 3): å»é‡é€Ÿç‡ vs F1çš„å¯è§†åŒ–æ›²çº¿

#### 8.2 Threats to Validity - æ›´æ–°
- å¼ºè°ƒDedupShifté˜ˆå€¼æƒè¡¡å·²é€šè¿‡æ•æ„Ÿæ€§åˆ†æé‡åŒ–
- è¯´æ˜neural baselineçš„seedæ–¹å·®å·²é€šè¿‡å¤šseedæŠ¥å‘Šæ§åˆ¶

---

## æ€§èƒ½è€ƒè™‘

### è¿è¡Œæ—¶é—´ä¼°è®¡

| ä»»åŠ¡ | æ—¶é—´ | CPU/GPU |
|------|------|---------|
| `sensitivity_analysis_dedup` | ~30ç§’ | CPU (3 thresholds Ã— 2 datasets) |
| `distilbert_multiseed` | ~10-15åˆ†é’Ÿ (CPU) / ~2åˆ†é’Ÿ (GPU) | 3ä¸ªseeds Ã— DistilBERTè®­ç»ƒ |
| `generate_sensitivity_tables` | ~5ç§’ | CPU |

**å»ºè®®**: 
- å¦‚æœCPUè¾ƒæ…¢ï¼ŒDedupShiftæ•æ„Ÿæ€§åˆ†æå¯å¿«é€Ÿå®Œæˆ
- DistilBERTå¤šseedè®­ç»ƒå¯é€‰æ‹©åœ¨GPUæœåŠ¡å™¨ä¸Šè¿è¡Œï¼ˆ`--device cuda`ï¼‰

### GPUæ”¯æŒ

```bash
# åœ¨GPUä¸Šè¿è¡Œå¤šseed DistilBERT
python src/train_distilbert_multiseed.py \
  ... \
  --device cuda  # æˆ– 'mps' for Apple Silicon
```

---

## é›†æˆåˆ°è®ºæ–‡æäº¤çš„å»ºè®®

### è¡¨æ ¼å’Œå›¾è¡¨ä½ç½®
```
paper/
â”œâ”€â”€ tables/
â”‚   â”œâ”€â”€ sensitivity_dedup_threshold.tex    # æ–°å¢
â”‚   â”œâ”€â”€ distilbert_multiseed.tex           # æ–°å¢
â”‚   â”œâ”€â”€ cross_domain_table_dedup.tex       # æ—¢æœ‰
â”‚   â”œâ”€â”€ dedup_effect.tex
â”‚   â””â”€â”€ ...
â”œâ”€â”€ figs/
â”‚   â”œâ”€â”€ fig_sensitivity_dedup_threshold.png # æ–°å¢
â”‚   â””â”€â”€ ...
â””â”€â”€ main.tex  # æ–°å¢ Section 7
```

### è®ºæ–‡æè¿°å»ºè®®

**åœ¨Resultsæˆ–Experimental Setupä¸­**:
> "We analyze the robustness of DedupShift to the choice of Hamming threshold $h_{\text{thresh}} \in \{2,3,4\}$ by measuring deduplication rates and downstream model performance (Table X, Fig. Y). To ensure training stability of our neural baseline (DistilBERT), we train with three seeds (0, 1, 2) and report mean $\pm$ std F1 scores (Table Z), confirming high in-domain stability but substantial cross-domain variance."

---

## æ•…éšœæ’æŸ¥

### `sensitivity_analysis_dedup` å¤±è´¥
- âœ… æ£€æŸ¥ `dataset/processed/train.csv` å­˜åœ¨
- âœ… æ£€æŸ¥ `src/dedup_split.py` å¯ç”¨
- âœ… è‹¥ä½¿ç”¨è‡ªå®šä¹‰pathsï¼Œæ›´æ–°è„šæœ¬ä¸­çš„ç¡¬ç¼–ç è·¯å¾„

### `distilbert_multiseed` å¤±è´¥
- âœ… æ£€æŸ¥ `transformers`, `torch` å·²å®‰è£…
- âœ… æ£€æŸ¥ GPU å¯ç”¨ï¼ˆè‹¥ä½¿ç”¨ `--device cuda`ï¼‰
- âœ… æ£€æŸ¥ `dataset/dedup/processed/all.csv` å­˜åœ¨
- âœ… è‹¥å†…å­˜ä¸è¶³ï¼Œå‡å° `--batch` å¤§å°

### `generate_sensitivity_tables` å¤±è´¥
- âœ… æ£€æŸ¥ `matplotlib` å·²å®‰è£…
- âœ… æ£€æŸ¥ `results/sensitivity_dedup_summary.csv` å­˜åœ¨
- âœ… æ£€æŸ¥ `paper/tables/` å’Œ `paper/figs/` ç›®å½•å­˜åœ¨

---

## å‚è€ƒ

- **è®ºæ–‡**: [paper/main.tex](paper/main.tex) - å®Œæ•´è®ºæ–‡ï¼ˆå«æ–°Sectionï¼‰
- **Makefile**: [Makefile](Makefile) - åŒ…å«æ‰€æœ‰targetså®šä¹‰
- **è„šæœ¬**:
  - [src/sensitivity_analysis_dedup.py](src/sensitivity_analysis_dedup.py)
  - [src/train_distilbert_multiseed.py](src/train_distilbert_multiseed.py)
  - [src/generate_sensitivity_tables.py](src/generate_sensitivity_tables.py)

---

**æ€»ç»“**: P2åŠŸèƒ½é€šè¿‡å®šé‡åˆ†æDedupShiftè¶…å‚ä»¥åŠå¤šseed neural baselineç»“æœï¼Œè¿›ä¸€æ­¥å¢å¼ºäº†è®ºæ–‡çš„å®éªŒå®Œæ•´æ€§å’ŒCCF-CæŠ•ç¨¿çš„ç«äº‰åŠ›ã€‚ğŸ¯
